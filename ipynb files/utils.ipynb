{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19747545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDailyVol(close, span0=100):\n",
    "    # daily vol, reindexed to cloes\n",
    "    df0 = close.index.searchsorted(close.index - pd.Timedelta(days=1))\n",
    "    df0 = df0[df0>0]\n",
    "    df0 = pd.Series(close.index[df0 - 1], index=close.index[close.shape[0] - df0.shape[0]:])\n",
    "    df0 = close.loc[df0.index] / close.loc[df0.values].values - 1 # daily returns\n",
    "    df0 = df0.ewm(span=span0).std()\n",
    "    return df0\n",
    "\n",
    "def getVerticalBarriers(close, tEvents, numDays):\n",
    "    t1 = close.index.searchsorted(tEvents+pd.Timedelta(days=numDays))\n",
    "    t1 = t1[t1 < close.shape[0]]\n",
    "    t1 = pd.Series(close.index[t1], index=tEvents[:t1.shape[0]]) # NaNs at the end\n",
    "    return t1\n",
    "\n",
    "def group_bars(series, bar_idx):\n",
    "    gg = series.groupby(bar_idx)\n",
    "    df = pd.DataFrame()\n",
    "    df['volume'] = gg['volume'].sum()\n",
    "    if 'Dollar Volume' in series.columns:\n",
    "        df['Dollar Volume'] = gg['Dollar Volume'].sum()\n",
    "    df['open'] = gg['open'].first()\n",
    "    df['low'] = gg['low'].first()\n",
    "    df['high'] = gg['high'].last()\n",
    "    df['close'] = gg['close'].last()\n",
    "    if 'rPrices' in series.columns:\n",
    "        df['rPrices'] = gg['rPrices'].last()\n",
    "    #df['Instrument'] = gg['Instrument'].first()\n",
    "    df['Time'] = gg.apply(lambda x:x.index[0])\n",
    "    df['Num Ticks'] = gg.size()\n",
    "    df = df.set_index(gg.apply(lambda x:x.index[0]))\n",
    "    return df\n",
    "\n",
    "def dollar_bars(series, bar_size=10000 * 3000):\n",
    "    series = series.copy(deep=True)\n",
    "    series['Dollar Volume'] = (series['volume'] * series['close'])\n",
    "    series['Cum Dollar Volume'] = series['Dollar Volume'].cumsum()\n",
    "    bar_idx = (series['Cum Dollar Volume'] / bar_size).round(0).astype(int).values\n",
    "    return group_bars(series, bar_idx)\n",
    "\n",
    "def cusum(gRaw, h):\n",
    "    tEvents, sPos, sNeg = [], 0, 0\n",
    "    diff = gRaw.diff()\n",
    "    for i in diff.index[1:]:\n",
    "        sPos, sNeg = max(0, sPos + diff.loc[i]), min(0, sNeg + diff.loc[i])\n",
    "        if sNeg < -h:\n",
    "            sNeg = 0\n",
    "            tEvents.append(i)\n",
    "        elif sPos > h:\n",
    "            sPos = 0\n",
    "            tEvents.append(i)\n",
    "    return pd.DatetimeIndex(tEvents)\n",
    "\n",
    "def mpPandasObj(func,pdObj,numThreads=24,mpBatches=1,linMols=True,**kargs):\n",
    "    '''\n",
    "    Parallelize jobs, return a dataframe or series\n",
    "    + func: function to be parallelized. Returns a DataFrame\n",
    "    + pdObj[0]: Name of argument used to pass the molecule\n",
    "    + pdObj[1]: List of atoms that will be grouped into molecules\n",
    "    + kwds: any other argument needed by func\n",
    "    Example: df1=mpPandasObj(func,('molecule',df0.index),24,**kwds)\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    #if linMols:parts=linParts(len(argList[1]),numThreads*mpBatches)\n",
    "    #else:parts=nestedParts(len(argList[1]),numThreads*mpBatches)\n",
    "    if linMols:parts=linParts(len(pdObj[1]),numThreads*mpBatches)\n",
    "    else:parts=nestedParts(len(pdObj[1]),numThreads*mpBatches)\n",
    "\n",
    "    jobs=[]\n",
    "    for i in range(1,len(parts)):\n",
    "        job={pdObj[0]:pdObj[1][parts[i-1]:parts[i]],'func':func}\n",
    "        job.update(kargs)\n",
    "        jobs.append(job)\n",
    "    if numThreads==1:out=processJobs_(jobs)\n",
    "    else: out=processJobs(jobs,numThreads=numThreads)\n",
    "    if isinstance(out[0],pd.DataFrame):df0=pd.DataFrame()\n",
    "    elif isinstance(out[0],pd.Series):df0=pd.Series()\n",
    "    else:return out\n",
    "    for i in out:df0=df0.append(i)\n",
    "    df0=df0.sort_index()\n",
    "    return df0\n",
    "\n",
    "def linParts(numAtoms,numThreads):\n",
    "    # partition of atoms with a single loop\n",
    "    parts=np.linspace(0,numAtoms,min(numThreads,numAtoms)+1)\n",
    "    parts=np.ceil(parts).astype(int)\n",
    "    return parts\n",
    "\n",
    "def processJobs_(jobs):\n",
    "    # Run jobs sequentially, for debugging\n",
    "    out=[]\n",
    "    for job in jobs:\n",
    "        out_=expandCall(job)\n",
    "        out.append(out_)\n",
    "    return out\n",
    "\n",
    "def expandCall(kargs):\n",
    "    # Expand the arguments of a callback function, kargs['func']\n",
    "    func=kargs['func']\n",
    "    del kargs['func']\n",
    "    out=func(**kargs)\n",
    "    return out\n",
    "\n",
    "\n",
    "def getEvents(close, tEvents, ptSl, trgt, minRet, numThreads=1, t1=False, side=None):\n",
    "    # 1) get target\n",
    "    trgt = trgt.loc[tEvents]\n",
    "    trgt = trgt[trgt > minRet]\n",
    "    # 2) get t1 (max holding period)\n",
    "    if t1 is False:\n",
    "        t1 = pd.Series(pd.NaT, index=tEvents)\n",
    "    # 3) form events object, apply stop loss on t1\n",
    "    if side is None:\n",
    "        side_, ptSl_ = pd.Series(1.0, index=trgt.index), [ptSl[0], ptSl[0]]\n",
    "    else:\n",
    "        side_, ptSl_ = side.loc[trgt.index], ptSl[:2]\n",
    "    events = pd.concat({'t1': t1, 'trgt': trgt, 'side': side_}, axis=1).dropna(subset=[('trgt', 'close')])\n",
    "    df0 = mpPandasObj(func=applyPtSlOnT1, pdObj=('molecule', events.index), numThreads=numThreads, close=close, events=events, ptSl=ptSl_)\n",
    "    events['t1'] = df0.dropna(how='all').min(axis=1) # pd.min ignores NaN\n",
    "    if side is None:\n",
    "        events = events.drop('side', axis=1)\n",
    "\n",
    "    # store for later\n",
    "    events['pt'] = ptSl[0]\n",
    "    events['sl'] = ptSl[1]\n",
    "\n",
    "    return events\n",
    "    \n",
    "def applyPtSlOnT1(close, events, ptSl, molecule):\n",
    "    # apply stop loss/profit taking, if it takes place before t1 (end of event)\n",
    "    events_ = events.loc[molecule]\n",
    "    out = events_[['t1']].copy(deep=True)\n",
    "\n",
    "    if ptSl[0] > 0:\n",
    "        pt = ptSl[0] * events_['trgt']\n",
    "    else:\n",
    "        pt = pd.Series(index=events.index) # NaNs\n",
    "\n",
    "    if ptSl[1] > 0:\n",
    "        sl = - ptSl[1] * events_['trgt']\n",
    "    else:\n",
    "        sl = pd.Series(index=events.index) # 'mo NaNs\n",
    "    \n",
    "    for l, t1 in events_['t1'].fillna(close.index[-1]).iteritems():\n",
    "        if l==0: continue\n",
    "        df0 = close[l:t1] # path prices\n",
    "        df0 = (df0 / close[l] - 1) * events_.at[l, 'side'] # path returns\n",
    "        out.loc[l, 'sl'] = df0[df0<sl[l]].index.min() # earliest stop loss\n",
    "        out.loc[l, 'pt'] = df0[df0>pt[l]].index.min() # earliest profit take\n",
    "    return out\n",
    "\n",
    "def getBins(events, close):\n",
    "    # 1) prices aligned with events\n",
    "    events_ = events.dropna(subset=[('t1',0)])\n",
    "    px = events_.index.union(events_[('t1',0)].values).drop_duplicates()\n",
    "    px = close.reindex(px, method='bfill')\n",
    "    # 2) create out object\n",
    "    out = pd.DataFrame(index=events_.index)\n",
    "    out['ret'] = (px.loc[events_[('t1',0)].values].values / px.loc[events_.index]) - 1\n",
    "    out['bin'] = np.sign(out['ret'])\n",
    "    return out\n",
    "\n",
    "def getWeights(d,size):\n",
    "    # thres>0 drops insignificant weights\n",
    "    w=[1.]\n",
    "    for k in range(1,size):\n",
    "        w_=-w[-1]/k*(d-k+1)\n",
    "        w.append(w_) \n",
    "    w=np.array(w[::-1]).reshape(-1,1) \n",
    "    return w\n",
    "\n",
    "def plotWeights(dRange,nPlots,size):\n",
    "    w=pd.DataFrame()\n",
    "    for d in np.linspace(dRange[0],dRange[1],nPlots):\n",
    "        w_=getWeights(d,size=size) \n",
    "        w_=pd.DataFrame(w_,index=range(w_.shape[0])[::-1],columns=[d]) \n",
    "        w=w.join(w_,how='outer')\n",
    "    ax=w.plot()\n",
    "    ax.legend(loc='upper left');mpl.show() \n",
    "    return\n",
    "\n",
    "def fracDiff(series,d,thres=.01): \n",
    "    '''\n",
    "    Increasing width window, with treatment of NaNs\n",
    "    Note 1: For thres=1, nothing is skipped.\n",
    "    Note 2: d can be any positive fractional, not necessarily bounded [0,1]. \n",
    "    '''\n",
    "    #1) Compute weights for the longest series \n",
    "    w=getWeights(d,series.shape[0])\n",
    "    #2) Determine initial calcs to be skipped based on weight-loss threshold \n",
    "    w_=np.cumsum(abs(w))\n",
    "    w_/=w_[-1]\n",
    "    skip=w_[w_>thres].shape[0]\n",
    "    #3) Apply weights to values\n",
    "    df={}\n",
    "    for name in series.columns:\n",
    "        seriesF,df_=series[[name]].fillna(method='ffill').dropna(),pd.Series() \n",
    "        for iloc in range(skip,seriesF.shape[0]):\n",
    "            loc=seriesF.index[iloc]\n",
    "            if not np.isfinite(series.loc[loc,name]):\n",
    "                continue # exclude NAs \n",
    "            df_[loc]=np.dot(w[-(iloc+1):,:].T,seriesF.loc[:loc])[0,0]\n",
    "        df[name]=df_.copy(deep=True) \n",
    "    df=pd.concat(df,axis=1)\n",
    "    return df\n",
    "\n",
    "def getWeights_FFD(d, thres):\n",
    "    w, k = [1.0], 1\n",
    "    while True:\n",
    "        w_ = -w[-1] / k * (d - k + 1)\n",
    "        if abs(w_) < thres:\n",
    "            break\n",
    "        w.append(w_)\n",
    "        k += 1\n",
    "    return np.array(w[::-1]).reshape(-1, 1)\n",
    "\n",
    "def fracDiff_FFD(series,d,thres=1e-5): \n",
    "    '''\n",
    "    Constant width window (new solution)\n",
    "    Note 1: thres determines the cut-off weight for the window\n",
    "    Note 2: d can be any positive fractional, not necessarily bounded [0,1]. \n",
    "        '''\n",
    "    #1) Compute weights for the longest series\n",
    "    w=getWeights_FFD(d,thres)\n",
    "    width=len(w)-1\n",
    "    #2) Apply weights to values\n",
    "    df={}\n",
    "    for name in series.columns: \n",
    "        seriesF,df_=series[[name]].fillna(method='ffill').dropna(),pd.Series() \n",
    "        for iloc1 in range(width,seriesF.shape[0]):\n",
    "            loc0,loc1=seriesF.index[iloc1-width],seriesF.index[iloc1]\n",
    "            if not np.isfinite(series.loc[loc1,name]):continue # exclude NAs \n",
    "            df_[loc1]=np.dot(w.T,seriesF.loc[loc0:loc1])[0,0]\n",
    "        df[name]=df_.copy(deep=True) \n",
    "    df=pd.concat(df,axis=1)\n",
    "    return df\n",
    "\n",
    "def dropLabels(events, mitPct=0.05):\n",
    "    # apply weights, drop labels with insufficient examples\n",
    "    while True:\n",
    "        df0 = events['bin'].value_counts(normalize=True)[::-1]\n",
    "        if df0.min() > mitPct or df0.shape[0] < 3:\n",
    "            break\n",
    "        print(\"Dropped label\", df0.argmin(), df0.min())\n",
    "        events = events[events['bin'] != df0.argmin()]\n",
    "    return events\n",
    "\n",
    "\n",
    "def barrier_touched(out_df, events):\n",
    "    # We'll graciously use the barrier_touched method from\n",
    "    # https://github.com/hudson-and-thames/mlfinlab/blob/master/mlfinlab/labeling/labeling.py#L164\n",
    "    store = []\n",
    "    for date_time, values in out_df.iterrows():\n",
    "        ret = values['ret']\n",
    "        target = values['trgt']\n",
    "\n",
    "        pt_level_reached = ret > target * events.loc[date_time, 'pt']\n",
    "        sl_level_reached = ret < -target * events.loc[date_time, 'sl']\n",
    "\n",
    "        if ret > 0.0 and pt_level_reached.item():\n",
    "            # Top barrier reached\n",
    "            store.append(1)\n",
    "        elif ret < 0.0 and sl_level_reached.item():\n",
    "            # Bottom barrier reached\n",
    "            store.append(-1)\n",
    "        else:\n",
    "            # Vertical barrier reached\n",
    "            store.append(0)\n",
    "\n",
    "    # Save to 'bin' column and return\n",
    "    out_df['bin'] = store\n",
    "    return out_df\n",
    "\n",
    "def getMetaBins(events, close):\n",
    "    '''\n",
    "    Compute event's outcome (including side information, if provided).\n",
    "    events is a DataFrame where:\n",
    "    -events.index is event's starttime\n",
    "    -events['t1'] is event's endtime\n",
    "    -events['trgt'] is event's target\n",
    "    -events['side'] (optional) implies the algo's position side\n",
    "    Case 1: ('side' not in events): bin in (-1, 1) <- label by price action\n",
    "    Case 2: ('side' in events): bin in (0, 1) <- label by pnl (meta-labeling)\n",
    "    '''\n",
    "    # 1) prices aligned with events\n",
    "    events_ = events.dropna(subset=[('t1',0)])\n",
    "    px = events_.index.union(events_[('t1',0)].values).drop_duplicates()\n",
    "    px = close.reindex(px, method='bfill')\n",
    "    # 2) create out object\n",
    "    out = pd.DataFrame(index=events_.index)\n",
    "\n",
    "    out['ret'] = px.loc[events_[('t1',0)].values].values / px.loc[events_.index] - 1\n",
    "    if 'side' in events_:\n",
    "        out['ret'] *= events_['side']  # meta-labeling\n",
    "\n",
    "    out['trgt'] = events_['trgt']\n",
    "    out = barrier_touched(out, events)\n",
    "\n",
    "    if 'side' in events_:\n",
    "        out.loc[out['ret'] <= 0, 'bin'] = 0\n",
    "        \n",
    "    if 'side' in events_:\n",
    "        out['side'] = events['side']\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
