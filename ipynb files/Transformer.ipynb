{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb3d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26557419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0, 2, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c2c06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_ts(path, dataset, tensor_format=True):\n",
    "    path = path + \"raw/\" + dataset + \"/\"\n",
    "    x_train = np.load(path + 'X_train.npy')\n",
    "    y_train = np.load(path + 'y_train.npy')\n",
    "    x_test = np.load(path + 'X_test.npy')\n",
    "    y_test = np.load(path + 'y_test.npy')\n",
    "    ts = np.concatenate((x_train, x_test), axis=0)\n",
    "    ts = np.transpose(ts, axes=(0, 1, 2))\n",
    "    labels = np.concatenate((y_train, y_test), axis=0)\n",
    "    nclass = int(np.amax(labels)) + 1\n",
    "\n",
    "\n",
    "    train_size = y_train.shape[0]\n",
    "\n",
    "    total_size = labels.shape[0]\n",
    "    idx_train = range(train_size)\n",
    "    idx_val = range(train_size, total_size)\n",
    "    idx_test = range(train_size, total_size)\n",
    "\n",
    "    if tensor_format:\n",
    "        # features = torch.FloatTensor(np.array(features))\n",
    "        ts = torch.FloatTensor(np.array(ts))\n",
    "        labels = torch.LongTensor(labels)\n",
    "\n",
    "        idx_train = torch.LongTensor(idx_train)\n",
    "        idx_val = torch.LongTensor(idx_val)\n",
    "        idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return ts, labels, idx_train, idx_val, idx_test, nclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a770e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"ArticularyWordRecognition\",\"AtrialFibrilation\",\"BasicMotions\",\n",
    "\"CharacterTrajectories\",\"Cricket\",\"DuckDuckGeese\",\"EigenWorms\",\"Epilepsy\",\"ERing\",\"EthanolConcentration\",\n",
    "\"FaceDetection\",\"FingerMovements\",\"HandMovementDirection\",\"Handwriting\",\"Heartbeat\",\"InsectWingbeat\",\n",
    "\"JapaneseVowels\",\"Libras\",\"LSST\",\"MotorImagery\",\"NATOPS\",\"PEMS-SF\",\"PenDigits\",\"Phoneme\",\"RacketSports\",\n",
    "\"SelfRegulationSCP1\",\"SelfRegulationSCP2\",\"SpokenArabicDigits\",\"StandWalkJump\",\"UWaveGestureLibrary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e804baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5902ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    raise ValueError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "\n",
    "    \n",
    "def get_pos_encoder(pos_encoding):\n",
    "    if pos_encoding == \"learnable\":\n",
    "        return LearnablePositionalEncoding\n",
    "    elif pos_encoding == \"fixed\":\n",
    "        return FixedPositionalEncoding\n",
    "\n",
    "    raise NotImplementedError(\"pos_encoding should be 'learnable'/'fixed', not '{}'\".format(pos_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b3593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedPositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=1024).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n",
    "        super(FixedPositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # positional encoding\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = scale_factor * pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)  # this stores the variable in the state_dict (used for non-trainable variables)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "408699b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBatchNormEncoderLayer(nn.modules.Module):\n",
    "    r\"\"\"This transformer encoder layer block is made up of self-attn and feedforward network.\n",
    "    It differs from TransformerEncoderLayer in torch/nn/modules/transformer.py in that it replaces LayerNorm\n",
    "    with BatchNorm.\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerBatchNormEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = BatchNorm1d(d_model, eps=1e-5)  # normalizes each feature across batch samples and time steps\n",
    "        self.norm2 = BatchNorm1d(d_model, eps=1e-5)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerBatchNormEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)  # (seq_len, batch_size, d_model)\n",
    "        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        # src = src.reshape([src.shape[0], -1])  # (batch_size, seq_length * d_model)\n",
    "        src = self.norm1(src)\n",
    "        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)  # (seq_len, batch_size, d_model)\n",
    "        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        src = self.norm2(src)\n",
    "        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ce839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a172ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSTransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, feat_dim, max_len, d_model, n_heads, num_layers, dim_feedforward, dropout=0.1,\n",
    "                 pos_encoding='fixed', activation='gelu', norm='BatchNorm', freeze=False):\n",
    "        super(TSTransformerEncoder, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.feat_dim = feat_dim\n",
    "        \n",
    "\n",
    "        self.project_inp = nn.Linear(feat_dim, d_model)\n",
    "        self.pos_enc = get_pos_encoder(pos_encoding)(d_model, dropout=dropout*(1.0 - freeze), max_len=max_len)\n",
    "\n",
    "        if norm == 'LayerNorm':\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, self.n_heads, dim_feedforward, dropout*(1.0 - freeze), activation=activation)\n",
    "        else:\n",
    "            encoder_layer = TransformerBatchNormEncoderLayer(d_model, self.n_heads, dim_feedforward, dropout*(1.0 - freeze), activation=activation)\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, feat_dim)\n",
    "\n",
    "        self.act = _get_activation_fn(activation)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, X, padding_masks):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: (batch_size, seq_length, feat_dim) torch tensor of masked features (input)\n",
    "            padding_masks: (batch_size, seq_length) boolean tensor, 1 means keep vector at this position, 0 means padding\n",
    "        Returns:\n",
    "            output: (batch_size, seq_length, feat_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # permute because pytorch convention for transformers is [seq_length, batch_size, feat_dim]. padding_masks [batch_size, feat_dim]\n",
    "        inp = X.permute(1, 0, 2)\n",
    "        inp = self.project_inp(inp) * math.sqrt(\n",
    "            self.d_model)  # [seq_length, batch_size, d_model] project input vectors to d_model dimensional space\n",
    "        inp = self.pos_enc(inp)  # add positional encoding\n",
    "        # NOTE: logic for padding masks is reversed to comply with definition in MultiHeadAttention, TransformerEncoderLayer\n",
    "        output = self.transformer_encoder(inp, src_key_padding_mask=~padding_masks)  # (seq_length, batch_size, d_model)\n",
    "        output = self.act(output)  # the output transformer encoder/decoder embeddings don't include non-linearity\n",
    "        output = output.permute(1, 0, 2)  # (batch_size, seq_length, d_model)\n",
    "        output = self.dropout1(output)\n",
    "        # Most probably defining a Linear(d_model,feat_dim) vectorizes the operation over (seq_length, batch_size).\n",
    "        output = self.output_layer(output)  # (batch_size, seq_length, feat_dim)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e56cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSTransformerEncoderClassiregressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplest classifier/regressor. Can be either regressor or classifier because the output does not include\n",
    "    softmax. Concatenates final layer embeddings and uses 0s to ignore padding embeddings in final output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feat_dim, max_len, d_model, n_heads, num_layers, dim_feedforward, num_classes,\n",
    "                 dropout=0.1, pos_encoding='fixed', activation='gelu', norm='BatchNorm', freeze=False):\n",
    "        super(TSTransformerEncoderClassiregressor, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.project_inp = nn.Linear(feat_dim, d_model)\n",
    "        self.pos_enc = get_pos_encoder(pos_encoding)(d_model, dropout=dropout*(1.0 - freeze), max_len=max_len)\n",
    "\n",
    "        if norm == 'LayerNorm':\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, self.n_heads, dim_feedforward, dropout*(1.0 - freeze), activation=activation)\n",
    "        else:\n",
    "            encoder_layer = TransformerBatchNormEncoderLayer(d_model, self.n_heads, dim_feedforward, dropout*(1.0 - freeze), activation=activation)\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.act = _get_activation_fn(activation)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.output_layer = self.build_output_module(d_model, max_len, num_classes)\n",
    "\n",
    "    def build_output_module(self, d_model, max_len, num_classes):\n",
    "        output_layer = nn.Linear(d_model * max_len, num_classes)\n",
    "        # no softmax (or log softmax), because CrossEntropyLoss does this internally. If probabilities are needed,\n",
    "        # add F.log_softmax and use NLLoss\n",
    "        return output_layer\n",
    "\n",
    "    def forward(self, X, padding_masks):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: (batch_size, seq_length, feat_dim) torch tensor of masked features (input)\n",
    "            padding_masks: (batch_size, seq_length) boolean tensor, 1 means keep vector at this position, 0 means padding\n",
    "        Returns:\n",
    "            output: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # permute because pytorch convention for transformers is [seq_length, batch_size, feat_dim]. padding_masks [batch_size, feat_dim]\n",
    "        inp = X.permute(1, 0, 2)\n",
    "        inp = self.project_inp(inp) * math.sqrt(\n",
    "            self.d_model)  # [seq_length, batch_size, d_model] project input vectors to d_model dimensional space\n",
    "        inp = self.pos_enc(inp)  # add positional encoding\n",
    "        # NOTE: logic for padding masks is reversed to comply with definition in MultiHeadAttention, TransformerEncoderLayer\n",
    "        output = self.transformer_encoder(inp, src_key_padding_mask=~padding_masks)  # (seq_length, batch_size, d_model)\n",
    "        output = self.act(output)  # the output transformer encoder/decoder embeddings don't include non-linearity\n",
    "        output = output.permute(1, 0, 2)  # (batch_size, seq_length, d_model)\n",
    "        output = self.dropout1(output)\n",
    "\n",
    "        # Output\n",
    "        output = output * padding_masks.unsqueeze(-1)  # zero-out padding embeddings\n",
    "        output = output.reshape(output.shape[0], -1)  # (batch_size, seq_length * d_model)\n",
    "        output = self.output_layer(output)  # (batch_size, num_classes)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437b1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fec1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sktime.utils.data_io import load_from_tsfile_to_dataframe as ts2df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960dd736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c289db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_, labels_, idx_train, idx_val, idx_test, nclass = \\\n",
    "                        load_raw_ts(\"/Users/abhijitdeshpande/Documents/Thesis/tapnet_converted_data/tapnet_converted_data/data/\",\n",
    "                                                                      dataset = 'NATOPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a01daae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_[idx_train]\n",
    "labels = labels_[idx_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f7b2b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([180, 51, 24])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df7c4776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([180, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5fa9462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Class: 6\n"
     ]
    }
   ],
   "source": [
    "n_class = np.unique(labels).__len__()\n",
    "print('Number of Class:', n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c61ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dim = features.shape[2]\n",
    "max_len  = features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72c86580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e2e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36741373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geom_noise_mask_single(L, lm, masking_ratio):\n",
    "    \"\"\"\n",
    "    Randomly create a boolean mask of length `L`, consisting of subsequences of average length lm, masking with 0s a `masking_ratio`\n",
    "    proportion of the sequence L. The length of masking subsequences and intervals follow a geometric distribution.\n",
    "    Args:\n",
    "        L: length of mask and sequence to be masked\n",
    "        lm: average length of masking subsequences (streaks of 0s)\n",
    "        masking_ratio: proportion of L to be masked\n",
    "    Returns:\n",
    "        (L,) boolean numpy array intended to mask ('drop') with 0s a sequence of length L\n",
    "    \"\"\"\n",
    "    keep_mask = np.ones(L, dtype=bool)\n",
    "    p_m = 1 / lm  # probability of each masking sequence stopping. parameter of geometric distribution.\n",
    "    p_u = p_m * masking_ratio / (1 - masking_ratio)  # probability of each unmasked sequence stopping. parameter of geometric distribution.\n",
    "    p = [p_m, p_u]\n",
    "\n",
    "    # Start in state 0 with masking_ratio probability\n",
    "    state = int(np.random.rand() > masking_ratio)  # state 0 means masking, 1 means not masking\n",
    "    for i in range(L):\n",
    "        keep_mask[i] = state  # here it happens that state and masking value corresponding to state are identical\n",
    "        if np.random.rand() < p[state]:\n",
    "            state = 1 - state\n",
    "\n",
    "    return keep_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb906f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(lengths, max_len=None):\n",
    "    \"\"\"\n",
    "    Used to mask padded positions: creates a (batch_size, max_len) boolean mask from a tensor of sequence lengths,\n",
    "    where 1 means keep element at this position (time step)\n",
    "    \"\"\"\n",
    "    batch_size = lengths.numel()\n",
    "    max_len = max_len or lengths.max_val()  # trick works because of overloading of 'or' operator for non-boolean types\n",
    "    return (torch.arange(0, max_len, device=lengths.device)\n",
    "            .type_as(lengths)\n",
    "            .repeat(batch_size, 1)\n",
    "            .lt(lengths.unsqueeze(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75d5af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_mask(X, masking_ratio, lm=3, mode='separate', distribution='geometric', exclude_feats=None):\n",
    "    \"\"\"\n",
    "    Creates a random boolean mask of the same shape as X, with 0s at places where a feature should be masked.\n",
    "    Args:\n",
    "        X: (seq_length, feat_dim) numpy array of features corresponding to a single sample\n",
    "        masking_ratio: proportion of seq_length to be masked. At each time step, will also be the proportion of\n",
    "            feat_dim that will be masked on average\n",
    "        lm: average length of masking subsequences (streaks of 0s). Used only when `distribution` is 'geometric'.\n",
    "        mode: whether each variable should be masked separately ('separate'), or all variables at a certain positions\n",
    "            should be masked concurrently ('concurrent')\n",
    "        distribution: whether each mask sequence element is sampled independently at random, or whether\n",
    "            sampling follows a markov chain (and thus is stateful), resulting in geometric distributions of\n",
    "            masked squences of a desired mean length `lm`\n",
    "        exclude_feats: iterable of indices corresponding to features to be excluded from masking (i.e. to remain all 1s)\n",
    "    Returns:\n",
    "        boolean numpy array with the same shape as X, with 0s at places where a feature should be masked\n",
    "    \"\"\"\n",
    "    if exclude_feats is not None:\n",
    "        exclude_feats = set(exclude_feats)\n",
    "\n",
    "    if distribution == 'geometric':  # stateful (Markov chain)\n",
    "        if mode == 'separate':  # each variable (feature) is independent\n",
    "            mask = np.ones(X.shape, dtype=bool)\n",
    "            for m in range(X.shape[1]):  # feature dimension\n",
    "                if exclude_feats is None or m not in exclude_feats:\n",
    "                    mask[:, m] = geom_noise_mask_single(X.shape[0], lm, masking_ratio)  # time dimension\n",
    "        else:  # replicate across feature dimension (mask all variables at the same positions concurrently)\n",
    "            mask = np.tile(np.expand_dims(geom_noise_mask_single(X.shape[0], lm, masking_ratio), 1), X.shape[1])\n",
    "    else:  # each position is independent Bernoulli with p = 1 - masking_ratio\n",
    "        if mode == 'separate':\n",
    "            mask = np.random.choice(np.array([True, False]), size=X.shape, replace=True,\n",
    "                                    p=(1 - masking_ratio, masking_ratio))\n",
    "        else:\n",
    "            mask = np.tile(np.random.choice(np.array([True, False]), size=(X.shape[0], 1), replace=True,\n",
    "                                            p=(1 - masking_ratio, masking_ratio)), X.shape[1])\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb658ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compensate_masking(X, mask):\n",
    "    \"\"\"\n",
    "    Compensate feature vectors after masking values, in a way that the matrix product W @ X would not be affected on average.\n",
    "    If p is the proportion of unmasked (active) elements, X' = X / p = X * feat_dim/num_active\n",
    "    Args:\n",
    "        X: (batch_size, seq_length, feat_dim) torch tensor\n",
    "        mask: (batch_size, seq_length, feat_dim) torch tensor: 0s means mask and predict, 1s: unaffected (active) input\n",
    "    Returns:\n",
    "        (batch_size, seq_length, feat_dim) compensated features\n",
    "    \"\"\"\n",
    "\n",
    "    # number of unmasked elements of feature vector for each time step\n",
    "    num_active = torch.sum(mask, dim=-1).unsqueeze(-1)  # (batch_size, seq_length, 1)\n",
    "    # to avoid division by 0, set the minimum to 1\n",
    "    num_active = torch.max(num_active, torch.ones(num_active.shape, dtype=torch.int16))  # (batch_size, seq_length, 1)\n",
    "    return X.shape[-1] * X / num_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ec2b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_unsuperv(data, mask, max_len=None, mask_compensation=False):\n",
    "    \"\"\"Build mini-batch tensors from a list of (X, mask) tuples. Mask input. Create\n",
    "    Args:\n",
    "        data: len(batch_size) list of tuples (X, mask).\n",
    "            - X: torch tensor of shape (seq_length, feat_dim); variable seq_length.\n",
    "            - mask: boolean torch tensor of shape (seq_length, feat_dim); variable seq_length.\n",
    "        max_len: global fixed sequence length. Used for architectures requiring fixed length input,\n",
    "            where the batch length cannot vary dynamically. Longer sequences are clipped, shorter are padded with 0s\n",
    "    Returns:\n",
    "        X: (batch_size, padded_length, feat_dim) torch tensor of masked features (input)\n",
    "        targets: (batch_size, padded_length, feat_dim) torch tensor of unmasked features (output)\n",
    "        target_masks: (batch_size, padded_length, feat_dim) boolean torch tensor\n",
    "            0 indicates masked values to be predicted, 1 indicates unaffected/\"active\" feature values\n",
    "        padding_masks: (batch_size, padded_length) boolean tensor, 1 means keep vector at this position, 0 ignore (padding)\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(data)\n",
    "    features, masks = data, mask\n",
    "\n",
    "    # Stack and pad features and masks (convert 2D to 3D tensors, i.e. add batch dimension)\n",
    "    lengths = [X.shape[0] for X in features]  # original sequence length for each time series\n",
    "    if max_len is None:\n",
    "        max_len = max(lengths)\n",
    "    X = torch.zeros(batch_size, max_len, features[0].shape[-1])  # (batch_size, padded_length, feat_dim)\n",
    "    target_masks = torch.zeros_like(X,\n",
    "                                    dtype=torch.bool)  # (batch_size, padded_length, feat_dim) masks related to objective\n",
    "    for i in range(batch_size):\n",
    "        end = min(lengths[i], max_len)\n",
    "        X[i, :end, :] = features[i][:end, :]\n",
    "        target_masks[i, :end, :] = masks[i][:end, :]\n",
    "\n",
    "    targets = X.clone()\n",
    "    X = X * target_masks  # mask input\n",
    "    if mask_compensation:\n",
    "        X = compensate_masking(X, target_masks)\n",
    "\n",
    "    padding_masks = padding_mask(torch.tensor(lengths, dtype=torch.int16), max_len=max_len)  # (batch_size, padded_length) boolean tensor, \"1\" means keep\n",
    "    target_masks = ~target_masks  # inverse logic: 0 now means ignore, 1 means predict\n",
    "    return X, targets, target_masks, padding_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b00d7339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\" Masked MSE Loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction: str = 'mean'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.reduction = reduction\n",
    "        self.mse_loss = nn.MSELoss(reduction=self.reduction)\n",
    "\n",
    "    def forward(self,\n",
    "                y_pred: torch.Tensor, y_true: torch.Tensor, mask: torch.BoolTensor) -> torch.Tensor:\n",
    "        \"\"\"Compute the loss between a target value and a prediction.\n",
    "        Args:\n",
    "            y_pred: Estimated values\n",
    "            y_true: Target values\n",
    "            mask: boolean tensor with 0s at places where values should be ignored and 1s where they should be considered\n",
    "        Returns\n",
    "        -------\n",
    "        if reduction == 'none':\n",
    "            (num_active,) Loss for each active batch element as a tensor with gradient attached.\n",
    "        if reduction == 'mean':\n",
    "            scalar mean loss over batch as a tensor with gradient attached.\n",
    "        \"\"\"\n",
    "\n",
    "        # for this particular loss, one may also elementwise multiply y_pred and y_true with the inverted mask\n",
    "        masked_pred = torch.masked_select(y_pred, mask)\n",
    "        masked_true = torch.masked_select(y_true, mask)\n",
    "\n",
    "        return self.mse_loss(masked_pred, masked_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66e93fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in features:\n",
    "    data.append(noise_mask(i, 0.15))\n",
    "    \n",
    "data_test = []\n",
    "for i in features_[idx_val]:\n",
    "    data_test.append(noise_mask(i, 0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef53923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, targets_test, target_masks_test, padding_masks_test = \\\n",
    "                        collate_unsuperv(features_[idx_val], torch.tensor(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efc1a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, targets, target_masks, padding_masks = collate_unsuperv(features, torch.tensor(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf22e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "d7ad96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSTransformerEncoderClassiregressor(feat_dim,max_len,128,8,5,750,n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "b7d688ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "8a4c187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    input_ = model(features,padding_masks)\n",
    "    target = labels.reshape(features.shape[0],)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    output = loss(input_, target)\n",
    "    optimizer.zero_grad()\n",
    "    output.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=4.0)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "ecd9543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.argmax(model(features,padding_masks), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484aad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "14160a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = labels.reshape(features.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "3ac564ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "f4f2545c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(p,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4094d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "b5bddd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = model(features_[idx_val],padding_masks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "d145d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_l = labels_[idx_val].reshape(pre.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "25a1f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = torch.argmax(pre,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "07829f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9055555555555556"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pre_l,pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e06c8a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_masks = target_masks * padding_masks.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe66aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d037c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcaede73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrain = TSTransformerEncoder(feat_dim,max_len,128,16,3,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "168650f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsuper_pretrain = model_pretrain(X,padding_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acbcc12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_unsuper = nn.MSELoss()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da39cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_pred = torch.masked_select(unsuper_pretrain,target_masks)\n",
    "masked_true = torch.masked_select(targets, target_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11ebcb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):   \n",
    "    input_ = model_pretrain(X,padding_masks)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    output = loss_unsuper(masked_pred, masked_true)\n",
    "    output.backward(retain_graph=True)\n",
    "    optimizer.zero_grad()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=4.0)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "32bb80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSTransformerEncoderClassiregressor(feat_dim,max_len,128,16,3,128,n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "e041719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "b2a9331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_output = model_pretrain(features, padding_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "3fce45eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    input_ = model(pretrain_output,padding_masks)\n",
    "    target = labels.reshape(features.shape[0],)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    output = loss(input_, target)\n",
    "    optimizer.zero_grad()\n",
    "    output.backward(retain_graph=True)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=4.0)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "e577a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_= torch.argmax(model(pretrain_output,padding_masks), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "9bb1b9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pretrain_,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e02145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73413812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "9ceafb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_out_test = model_pretrain(features_[idx_val],padding_masks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "f03101e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_out_test = torch.argmax(model(pretrain_out_test,padding_masks_test), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "2354ad75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7555555555555555"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pretrain_out_test,pre_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6185e6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ee05f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "e1fdee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf',C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "765b3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_input = model(targets,padding_masks).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ccf0999d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(svm_input, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "9b45f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_output = model(features_[idx_val],padding_masks).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4caed97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pred = svm.predict(svm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "0b01bbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6444444444444445"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(svm_pred,pre_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfabcd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "b81ddac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "a76095d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        #'d_model':trial.set_user_attr('d_model',[16,32,64,128,256,512]),\n",
    "        #'n_heads':trial.set_user_attr('n_heads',[4,8]),\n",
    "        'num_layers':trial.suggest_int('num_layers',1,5),\n",
    "        'dim_feedforward':trial.suggest_int('dim_feedforward',64,1028)\n",
    "    }\n",
    "    model = TSTransformerEncoderClassiregressor(feat_dim,max_len,128,\n",
    "                                            8,params['num_layers'],\n",
    "                                            params['dim_feedforward'],n_class)\n",
    "    for i in range(100):\n",
    "        input_ = model(features,padding_masks)\n",
    "        target = labels.reshape(features.shape[0],)\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        output = loss(input_, target)\n",
    "        optimizer.zero_grad()\n",
    "        output.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=4.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    prediction = torch.argmax(model(features,padding_masks), 1)\n",
    "    label = labels.reshape(features.shape[0])\n",
    "    return accuracy_score(prediction,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff52359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e9cb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "dfa3e29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-04 19:49:53,893]\u001b[0m A new study created in memory with name: no-name-e793cbab-e360-4b1e-9283-3e1308410bce\u001b[0m\n",
      "\u001b[32m[I 2021-10-04 19:52:10,515]\u001b[0m Trial 0 finished with value: 0.9833333333333333 and parameters: {'num_layers': 3, 'dim_feedforward': 687}. Best is trial 0 with value: 0.9833333333333333.\u001b[0m\n",
      "\u001b[32m[I 2021-10-04 19:53:03,205]\u001b[0m Trial 1 finished with value: 0.9833333333333333 and parameters: {'num_layers': 1, 'dim_feedforward': 838}. Best is trial 0 with value: 0.9833333333333333.\u001b[0m\n",
      "\u001b[32m[I 2021-10-04 19:56:26,995]\u001b[0m Trial 2 finished with value: 0.9777777777777777 and parameters: {'num_layers': 4, 'dim_feedforward': 771}. Best is trial 0 with value: 0.9833333333333333.\u001b[0m\n",
      "\u001b[32m[I 2021-10-04 19:59:23,902]\u001b[0m Trial 3 finished with value: 0.9777777777777777 and parameters: {'num_layers': 3, 'dim_feedforward': 1000}. Best is trial 0 with value: 0.9833333333333333.\u001b[0m\n",
      "\u001b[32m[I 2021-10-04 20:15:51,852]\u001b[0m Trial 4 finished with value: 0.9833333333333333 and parameters: {'num_layers': 5, 'dim_feedforward': 972}. Best is trial 0 with value: 0.9833333333333333.\u001b[0m\n",
      "\u001b[32m[I 2021-10-04 20:16:15,816]\u001b[0m Trial 5 finished with value: 0.9666666666666667 and parameters: {'num_layers': 1, 'dim_feedforward': 145}. Best is trial 0 with value: 0.9833333333333333.\u001b[0m\n",
      "\u001b[32m[I 2021-10-04 20:19:46,753]\u001b[0m Trial 6 finished with value: 0.9833333333333333 and parameters: {'num_layers': 4, 'dim_feedforward': 838}. Best is trial 0 with value: 0.9833333333333333.\u001b[0m\n",
      "\u001b[32m[I 2021-10-04 20:21:36,542]\u001b[0m Trial 7 finished with value: 0.9833333333333333 and parameters: {'num_layers': 3, 'dim_feedforward': 469}. Best is trial 0 with value: 0.9833333333333333.\u001b[0m\n",
      "\u001b[32m[I 2021-10-04 20:23:53,344]\u001b[0m Trial 8 finished with value: 0.9833333333333333 and parameters: {'num_layers': 4, 'dim_feedforward': 405}. Best is trial 0 with value: 0.9833333333333333.\u001b[0m\n",
      "\u001b[32m[I 2021-10-04 20:27:58,914]\u001b[0m Trial 9 finished with value: 0.9944444444444445 and parameters: {'num_layers': 5, 'dim_feedforward': 748}. Best is trial 9 with value: 0.9944444444444445.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best trail:\n",
      "[0.9944444444444445]\n",
      "{'num_layers': 5, 'dim_feedforward': 748}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "print('best trail:')\n",
    "trial_ = study.best_trial\n",
    "print(trial_.values)\n",
    "print(trial_.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189f08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a6accd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344d326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selection import TechnicalIndicator, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8673f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa874d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81715829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501974b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de7f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734736d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fbd5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ffb51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc602f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf88bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c687ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
